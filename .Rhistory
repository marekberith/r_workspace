data
data[data$default == 'Yes',]$default = 1
data[data$default == 'No',]$default = 0
data[data$student == 'Yes',]$default = 1
data[data$student == 'No',]$default = 0
data
data <- read.csv('data/Default.csv')
data
data[data$default == 'Yes',]$default = 1
data[data$default == 'No',]$default = 0
data[data$student == 'Yes',]$student = 1
data[data$student == 'No',]$student = 0
data
library(tidymodels)
library(tidyverse)
library(magrittr)
library(tidymodels)
library(tidyverse)
library(magrittr)
data <- read.csv('data/Default.csv')
data
# Task 1
# binary encode
data[data$default == 'Yes',]$default <- 1
data[data$default == 'No',]$default <- 0
data[data$student == 'Yes',]$student <- 1
data[data$student == 'No',]$student <- 0
data
# add IDs
data$ID <-
1:5
data
1:nrow(data)
data
# add IDs
data$ID <- 1:nrow(data)
data
# Task 2
hist(data)
# Task 2
hist(data$balance)
hist(data$income)
qqplot(data$balance)
qqplot(y = data$balance)
qqplot(data$balance, data$student)
qqplot(data$balance, data$default)
qqplot(data$balance, data$student)
qqplot(data$balance, data$default)
qqplot(data$income, data$default)
qqplot(data$income, data$student)
qqplot(data$balance, data$default)
hist(data$income)
lines(density(data), col = "red")
lines(density(data$income), col = "red")
lines(density(x), col = 4, lwd = 2)
lines(density(data$income), col = 4, lwd = 2)
lines(density(data$income))
hist(data$income)
lines(density(data$income))
lines(density(data$income), bw = 0.1)
lines(density(data$income, bw = 0.1))
hist(data$income)
lines(density(data$income, bw = 0.1))
hist(data$income)
lines(density(data$income, bw = 0.1))
lines(density(data$income, bw = 0.01))
lines(density(data$income, bw = 0.001))
lines(density(data$income, bw = 1))
hist(data$income, density(data$income))
library(tidymodels)
library(tidyverse)
library(magrittr)
data <- read.csv('data/Default.csv')
data
# Task 1
# binary encode
data[data$default == 'Yes',]$default <- 1
data[data$default == 'No',]$default <- 0
data[data$student == 'Yes',]$student <- 1
data[data$student == 'No',]$student <- 0
data
# add IDs
data$ID <- 1:nrow(data)
data
hist(data$income, density(data$income))
# Task 2
hist(data$income, freq = FALSE, main = "Histogram with KDE Line", xlab = "Income")
lines(density(data$income), col = "red")
# Task 2
hist(data$income, main = "Histogram with KDE Line", xlab = "Income")
lines(density(data$income), col = "red")
lines(density(data$income))
# Task 2
hist(data$income, freq = F)
lines(density(data$income))
# Task 2
hist(data$balance, freq = F)
lines(density(data$balance))
lines(density(data$balance), col = 'blue')
lines(density(data$balance, bw = 0.1), col = 'blue')
# Task 2
hist(data$balance, freq = F)
lines(density(data$balance, bw = 0.1), col = 'blue')
lines(density(data$balance, bw = 1), col = 'blue')
lines(density(data$balance, bw = 100), col = 'blue')
lines(density(data$balance, bw = 1000), col = 'blue')
lines(density(data$balance, bw = 200), col = 'blue')
# Task 2
hist(data$balance, freq = F)
lines(density(data$balance, bw = 200), col = 'blue')
lines(density(data$balance, bw = 150), col = 'blue')
lines(density(data$balance, bw = 100), col = 'blue')
lines(density(data$balance, bw = 900), col = 'blue')
lines(density(data$balance, bw = 50), col = 'blue')
lines(density(data$balance, bw = 120), col = 'blue')
# Task 2
hist(data$balance, freq = F)
lines(density(data$balance, bw = 120), col = 'blue')
# Task 2
hist(data$balance, freq = F)
lines(density(data$balance, bw = 150), col = 'blue')
# Task 2
hist(data$balance, freq = F, breaks = 200)
# Task 2
hist(data$balance, freq = F, breaks = 500)
# Task 2
hist(data$balance, freq = F, breaks = 10)
# Task 2
hist(data$balance, freq = F, breaks = 50)
lines(density(data$balance, bw = 150), col = 'blue')
qqplot(data$balance, data$default)
# converting to binary variables to factors
data %<>% mutate(default = as_factor(default))
qqplot(data$balance, data$default)
data
data$default
qqplot(data$balance[data$default == 0], data$default)
qqplot(data$balance[data$default == 1], data$default)
qqnorm(data$balance[data$default == 1], data$default)
qqnorm(data$balance[data$default == 1])
qqnorm(data$balance)
# normal q-q plot
qqnorm(data$balance)
# normal q-q plot
qqnorm(data$balance)
lines(density(data$balance, bw = 150), col = 'blue')
qqline()
qqline(data$balance)
# normal q-q plot
qqnorm(data$balance)
# normal q-q plot
qqnorm(data$balance)
qqline(data$balance)
qqline(data$balance, col = 'blue')
# q-q plot
qqplot(data$balance[data$default == 1], data$default)
# q-q plot
qqplot(data$balance[data$default == 1], data$default)
# Task 2
# histogram
hist(data$balance, freq = F, breaks = 50)
lines(density(data$balance, bw = 150), col = 'blue')
# remove with 0 on balance
data <- data[data$balance != 0]
# Task 2
# histogram
hist(data$balance, freq = F, breaks = 50)
# remove with 0 on balance
data <- data[data$balance != 0,]
# Task 2
# histogram
hist(data$balance, freq = F, breaks = 50)
lines(density(data$balance, bw = 150), col = 'blue')
# q-q plot
qqplot(data$balance[data$default == 1], data$default)
# q-q plot
qqplot(data$balance[data$default == 1], data$default)
# q-q plot
qqplot(data$balance, data$default)
# q-q plot
qqplot(data$balance, data$default)
library(tidymodels)
library(tidyverse)
library(magrittr)
data <- read.csv('data/Default.csv')
data
# Task 1
# binary encode
data[data$default == 'Yes',]$default <- 1
data[data$default == 'No',]$default <- 0
data[data$student == 'Yes',]$student <- 1
data[data$student == 'No',]$student <- 0
data
# add IDs
data$ID <- 1:nrow(data)
data
# converting to binary variables to factors
data %<>% mutate(default = as_factor(default))
data %<>% mutate(student = as_factor(student))
# remove with 0 on balance
data <- data[data$balance != 0,]
# Task 2
# histogram
hist(data$balance[data$default == 0, ], freq = F, breaks = 50)
# Task 2
# histogram
hist(data$balance[data$default == 0, ], freq = F, breaks = 50)
# Task 2
# histogram
hist(data$balance[data$default == 0, ], freq = F, breaks = 50)
# Task 2
# histogram
hist(data$balance, freq = F, breaks = 50)
# Task 2
# histogram
hist(data[data$default == 0, ]$balance, freq = F, breaks = 50)
# Task 2
# histogram
hist(data[data$default == 0]$balance, freq = F, breaks = 50)
# Task 2
# histogram
hist(data[data$default == 0, ]$balance, freq = F, breaks = 50)
library(tidymodels)
library(tidyverse)
library(magrittr)
data <- read.csv('data/Default.csv')
data
# Task 1
# binary encode
data[data$default == 'Yes',]$default <- 1
data[data$default == 'No',]$default <- 0
data[data$student == 'Yes',]$student <- 1
data[data$student == 'No',]$student <- 0
data
# add IDs
data$ID <- 1:nrow(data)
data
# converting to binary variables to factors
data %<>% mutate(default = as_factor(default))
data %<>% mutate(student = as_factor(student))
# remove with 0 on balance
data <- data[data$balance != 0,]
# Task 2
# histogram
hist(data[data$default == 0, ]$balance, freq = F, breaks = 50)
lines(density(data[data$default == 0, ]$balance, bw = 150), col = 'blue')
# normal q-q plot
qqnorm(data[data$default == 0, ]$balance)
qqline(data[data$default == 0, ]$balance, col = 'blue')
# q-q plot
qqplot(data$balance[data$default == 0, ], data$default)
# q-q plot
qqplot(data[data$default == 0, ]$balance, data$default)
library('latexpdf')
#'# Neparametricke metody
#'neparametricke metody pouzivame tam, kde data nie su normalne
#'rozdelene, je ich malo, su nestandardne. Neparametricke testy
#'vyzaduju podmienku, ze data su z nejakeho spojiteho rozdelenia.
#'Su menej presne a menej citlive.
#'
#'Hypoteza H0 je v tvare
#'$H_0\quad \text{data su nahodne}\quad H_1\quad \text{nie su nahodne}$
#'Test serii (Wald Wofovitzov test), testujeme nahodnost dat.
#'Test je citlivy na trend, kniznica randtests
library(randtests)
install.packages('randtests')
#'# Neparametricke metody
#'neparametricke metody pouzivame tam, kde data nie su normalne
#'rozdelene, je ich malo, su nestandardne. Neparametricke testy
#'vyzaduju podmienku, ze data su z nejakeho spojiteho rozdelenia.
#'Su menej presne a menej citlive.
#'
#'Hypoteza H0 je v tvare
#'$H_0\quad \text{data su nahodne}\quad H_1\quad \text{nie su nahodne}$
#'Test serii (Wald Wofovitzov test), testujeme nahodnost dat.
#'Test je citlivy na trend, kniznica randtests
library(randtests)
v<- c(7.7, 7.8, 8.5, 7.8, 7.9, 9, 7.5, 8.2, 9.3, 8.1)
runs.test(v, plot=T)
#' Phodnota > 0.05, data su nahodne
runs.test(v, alternative = 'left.sided')
runs.test(v, alternative = 'right.sided')
#' Test kritickych bodov, bodov obratu (turning point test)
#' odhaluje periodicitu v datach
turning.point.test(v)
#' P hodnota > 0.05, nezamietam hypotezu o nahodnosti dat
turning.point.test(v, alternative = 'left.sided')
#'sme testovali tvrdenia o strednej hodnote, boli jednovyberove t.testy
#'a dvojvyberove t.testy - parove testy a neparove
#'# Znamienkovy test (sign test)
#'testujeme $H_0\quad \text{median} = x_0\quad H_1\quad
#'\text{median}\neq x_0$
#'Jedina podmienka kladena na data je, aby boli vyberom zo
#'spojiteho rozdelenia. Test ma malu silu, t.j. chyba druheho druhu
#'je velka (nezamietame H0 a pritom H0 neplati).
#'Testujte, ze data z predosleho prikladu maju median = 8, teda ze
#'priemerna rychlost ani po uprave sa nezmenila.
boxplot(v, horizontal = T)
hist(v)
#' pre symetricke data je sikmost nulova, spocitame este sikmost
library(moments)
skewness(v)
library(BSDA)
install.packages('BSDA')
library('BSDA')
library('BSDA')
install.packages('BSDA')
install.packages("BSDA")
library('BSDA')
SIGN.test(v, md=8)
#' P hodnota > 0.05 nezamietame hypotezu o tom, ze priemerna
#' rychlost sa nezmenila. Tento test mozeme pouzit aj
#' ako dvojvyberovy parovy. Testujeme, ze median rozdielov je rovny
#' nejakemu cislu, pouzijeme ho tam, kde je asymetria dat.
#' Ak pridame predpoklad, ze data su symetricke okolo medianu, tak
#' radsej pouzijem jednovyberovy Wilcoxonov test
#' -signed rank test. Otestujeme nase data, ci su symetricke a ak ano
#' testujeme tymto testom.
library(lawstat)
install.packages('lawstat')
#' P hodnota > 0.05 nezamietame hypotezu o tom, ze priemerna
#' rychlost sa nezmenila. Tento test mozeme pouzit aj
#' ako dvojvyberovy parovy. Testujeme, ze median rozdielov je rovny
#' nejakemu cislu, pouzijeme ho tam, kde je asymetria dat.
#' Ak pridame predpoklad, ze data su symetricke okolo medianu, tak
#' radsej pouzijem jednovyberovy Wilcoxonov test
#' -signed rank test. Otestujeme nase data, ci su symetricke a ak ano
#' testujeme tymto testom.
library(lawstat)
symmetry.test(v)
#' p hodnota >0.05, rad
wilcox.test(v)
#' p hodnota >0.05, radsej Wilcoxonov test
wilcox.test(v, mu=8)
#' P hodnota >0.05, nezamietam H0, priemerna rychlost sa nezmenila
#' Pri tradicnom opracovani suciastok sa dosahovali priemerne hodnoty
#' kvalitativnej vlastnosti 4.4, pokusne sa zavadza nova jednoduchsia
#' metoda opracovania suciastok. Hodnoty kvalitativnej vlastnosti su
#' v datovom subore x.
x<-c(4.5, 4.3, 4.1, 4.9, 4.6, 3.6, 4.7, 5.1, 4.8, 4, 3.7, 4.4,
4.9, 4.9, 5.2, 5.1, 4.7, 4.9, 4.6, 4.8)
boxplot(x)
symmetry.test(x)
#' Data su symetricke, teda jednoznacne WT.
wilcox.test(x, mu=4.4)
#' P hodnota>0.05, nezamietam hypotezu, ze novou metodou opracovane
#' suciastky maju rovnaku kvalitativnu vlastnost
#' Prakticky by sme mali na zaciatku overit normalitu dat a ak
#' su normalne rozdelene, tak t.test
library(nortest)
shapiro.test(x)
t.test(x, mu=4.4)
shapiro.test(x) #normalita
t.test(x, mu=4.4)
pred <- c(87,61,98,90,93,74,83,72,81,75,83)
po <- c(50,45,79,90,88,65,52,79,84,61,52)
#' Aj parametrickym testom nam vyslo, ze kvalitativna vlastnost ostala rovnaka.
#' Priklad pouzitia, ako parovy test.
#' Su dane casy v sekundach, pocas ktorych vyriesili kontrolne ulohy
#' ziaci pred a po specialnych cviceniach z pamatoveho pocitania. Zlepsili
#' cvicenia schopnost ziakov rychlejsie riesit ulohy?
#' Ak sa pytame, ci zlepsili, pred - po >= 0, teda negacia je <0 alternativa
#' bude less. Testy vyberam podla toho, ci rozdiely su alebo nie su
#' symetricke.
pred <- c(87,61,98,90,93,74,83,72,81,75,83)
po <- c(50,45,79,90,88,65,52,79,84,61,52)
rozdiel = pred - po
rozdiel
boxplot(rozdiel, horizontal = T)
boxplot(rozdiel, horizon)
boxplot(rozdiel, horizontal = T)
skewness(rozdiel)
symmetry.test(rozdiel)
wilcox.test(rozdiel, alternative = 'less')
#' Dvojvyberovy neparovy test, neparametricky, dvojvyberovy
#' Wilcoxonov test (Mann Whitney U test). Nulova hypoteza je
#' $H_0\quad F_X=F_Y$. Pred testom treba overit, ci sa rozdelenia aspon
#' priblizne rovnaju a tiez ci maju rovnaku disperziu. Ak su velke rozdiely,
#' tak dvojvyberovy Kolmogorov Smirnov test. Neparametricky test pre
#' rovnost disperzii Levene test, kniznica BSDA
#' Z produkcie dvoch firiem bolo nahodne vybratych n=10 a m=8 vyrobkov.
#' Nezavisli experti hodnotili ich kvalitu pridelenim bodov.
#' Datovy subor x, y. Testujte hypotezu, ze kvalita vyrobkov dvoch
#' firiem je rovnaka.
x<-c(420,560,600,490,550,570,340,480,510,460)
y<-c(400,420,580,470,470,500,520,530)
boxplot(x, y, col = c('blue', 'red'))
par(mfrow=c(1,2))
hist(x, breaks = 5)
hist(y, breaks = 5)
par(mfrow = c(1,1))
#' Uprava dat pre test
data <- data.frame('body' = c(x, y), 'firma' = rep(c(1,2), times = c(10,8)))
data
levene.test(data$body, data$firma)
#' Mozeme pouzit dvojvyberovy WT
wilcox.test(x,y)
#' P hodnota > 0.05, nezamietame hypotezu o rovnosti DF, teda aj
#' medianov, kvalita je rovnaka, este KS test
ks.test(x, y)
#' plati tvrdenie hore
#'# Kruskal Wallisov test, neparametricky ekvivalent k ANOVA. ANOVA
#'mala silne podmienky, normalita dat v triedach, rovnost disperzii.
#'Ak to nie je splnene, tak KW test. Pri ANOVE sme testovali,
#'ze stredne hodnoty sa rovnaju, tu testujeme, ze distribucne funkcie sa
#'rovnaju (a teda aj mediany). Zaznamenali sme vykony strojov troch znaciek.
#'Na hladine vyznamnosti $\alpha=0.05$ testujte hypotezu vykony strojov
#'su rovnake.
data<-data.frame("vykon"=c(53,47,46,61,55,52,58,54,51,51,49,54),
"stroj"=c(rep(1,3),rep(2,5),rep(3,4)))
boxplot(data$vykon~data$stroj)
boxplot(data$vykon~data$stroj, col=c('red', 'green', 'blue'))
#' Faktorizujeme
data$stroj <- factor(data$stroj)
data$stroj
kruskal.test(data$vykon, data$stroj)
#' P hodnota <0.05, zamietame hypotezu o rovnakej vykonnosti
#' Nasleduju post testy. ANOVA (Tukey, Scheffe), tu pouzijeme dunn.test
#' (aj kniznica sa tak vola)
library('dunn.test')
dunn.test(data$vykon, data$stroj)
dunn.test(data$vykon, data$stroj, altp = T)
dunn.test(data$vykon, data$stroj, altp = T, list = T)
#'# Dvojrozmerny datovy subor, korelacna analyza
#' Analyzujeme dvojrozmerny datovy subor. Ratame charakteristiky pre
#' jednorozmerne subory, pribudnu charakteristiky pre dvojrozmerny subor.
#' Priklad. Tabulka uvadza cenu pevnych diskov (y) ich kapacitu (x)
#' od vyrobcu ABC. Zratajte vektor strednych hodnot a disperzii.
x <- c(160, 250, 320, 500, 750, 900, 1000, 1500, 2000)
y <- c(789, 800, 851, 874, 1193, 1200, 1335, 1704, 2073)
c(mean(x), mean(y))
c(var(x), var(y))
nciu
library(latexpdf)
#' Dalej ratame charakteristiku dvojrozmerneho datoveho suboru kovarianciu
#' (Personova, Kendallova, Spearmanova). Je to cislo, ktore nam charakterizuje,
#' ci je medzi zlozkami linearna zavislost alebo nie, ci koreluju. Kym
#' neznormujeme toto cislo, tak mozeme povedat iba jedine, cov = 0 zlozky su
#' nekorelovanea ako cov != 0 su korelovane.
plot(x, y, type = 'b')
cov(x, y) #korelovane
cov(x, y, method = 'kendall')
cov(x, y, method = 'spearman')
#' Aby sme zistili, ako silno sa ovplyvnuju, je potrebne nromovat kovarianciu,
#' dostaneme korelacny koeficient, co je cislo medzi -1 a 1. Blizke -1, 1 silna,
#' silna korelacia, silna linearna zavislost. Blizke 0, nekorelovane a linearne
#' nezavisle iba pre normalne rozdelene data. Zlozitejsie zavislosti sa
#' nemusia odhalit. Pearsonov korelacny koeficient je citlivy na vybocujuce
#' hodnoty, vsetky naraz ratat.Pripravime funkciu korelacie.
cor(x, y)
korelacie <- function(x, y){c(co)}
korelacie <- function(x, y){c('Pearson' = cor(x, y),
'Kendall' = cor(x, y, method = 'kendall'),
'Spearman' = cor(x, y, method = 'spearman'))}
korelacie(x, y)
merne data ale musia byt
#' Testovat korelacny koeficient mozeme, dvojrozmerne data ale musia byt
#' normalne rozdelene, testujeme hypotezu o nulovosti korelacneho koeficientu.
#' $$H_0\quad \rho=0\quad H_1\quad \rho \neq 0$$
#' kniznice pre test normality
library(nortest)
library(mvnormtest)
install.packages('mvnormtest')
library(mvnormtest)
#' uprava dat, dvojrozmerny subor
data <- rbind(x, y)
data
mshapiro.test(data)
#' P hodnota > 0.05 nezamietam hypotezu o normalite dat mozeme testovat
#' nulovost korelacneho koeficienta
cor.test(x, y) #zamietam H0
core.test(x, y, method = 'Kendall')
cor.test(x, y, method = 'Kendall')
cor.test(x, y, method = 'kendall')
cor.test(x, y, method = 'spearman')
#' graf s viac informaciami, nacitame kniznice
library(ggplot2)
library(ggpubr)
data1 <- data.frame(x, y)
ggscatter(data1, x, y)
ggscatter(data1, 'x', 'y')
ggscatter(data1, 'x', 'y') +
geom_smooth()
ggscatter(data1, 'x', 'y', add = 'reg.line')
ggscatter(data1, 'x', 'y',
add = 'reg.line',
conf.int = T)
ggscatter(data1, 'x', 'y',
add = 'reg.line',
conf.int = T,
cor.coef = T)
zavisle a data so zlozitejsou
#' Nasiumulujeme linearne nezavisle, linearne zavisle a data so zlozitejsou
#' zavislostou, grafy a korelacne koeficienty
xx <- rnorm(100, 2, 4)
yy <- rnorm(100, 1, 1)
plot(xx, yy)
plot(xx, yy, main = 'Nezavisle data')
korelacie(xx, yy)
xxx <- 1:100+xx
yyy <- 2+3*xxx+rnorm(100, 2, 6)
yyy <- 2+3*xxx+rnorm(100, 2, 6)
plot(xxx, yyy, main = 'Zavisle data')
korelacie(xxx, yyy)
X1 <- 1:100
y1 <- sin(x1) + rnorm(100, 0, 0.05)
X1 <- 1:100
y1 <- sin(x1) + rnorm(100, 0, 0.05)
X1 <- 1:100
y1 <- sin(x1) + rnorm(100, 0, 0.05)
X1 <- 1:100
x1
